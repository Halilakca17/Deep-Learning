{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lP6JLo1tGNBg"
      },
      "source": [
        "# Artificial Neural Network"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gWZyYmS_UE_L"
      },
      "source": [
        "### Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "MxkJoQBkUIHC"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "ZaTwK7ojXr2F",
        "outputId": "0b27a96d-d11a-43e8-ab4b-87c1f01896fe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'2.12.0'"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf.__version__"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1E0Q3aoKUCRX"
      },
      "source": [
        "## Part 1 - Data Preprocessing"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cKWAkFVGUU0Z"
      },
      "source": [
        "### Importing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "MXUkhkMfU4wq"
      },
      "outputs": [],
      "source": [
        "dataset = pd.read_csv('Churn_Modelling.csv')\n",
        "X = dataset.iloc[:, 3:-1].values\n",
        "y = dataset.iloc[:, -1].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "colab_type": "code",
        "id": "VYP9cQTWbzuI",
        "outputId": "797e7a64-9bac-436a-8c9c-94437e5e7587"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[619 'France' 'Female' ... 1 1 101348.88]\n",
            " [608 'Spain' 'Female' ... 0 1 112542.58]\n",
            " [502 'France' 'Female' ... 1 0 113931.57]\n",
            " ...\n",
            " [709 'France' 'Female' ... 0 1 42085.58]\n",
            " [772 'Germany' 'Male' ... 1 0 92888.52]\n",
            " [792 'France' 'Female' ... 1 0 38190.78]]\n"
          ]
        }
      ],
      "source": [
        "print(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "38vKGE6Nb2RR",
        "outputId": "a815e42a-e0dd-4cb5-ab97-b17ead98fbc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1 0 1 ... 1 1 0]\n"
          ]
        }
      ],
      "source": [
        "print(y)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "N6bQ0UgSU-NJ"
      },
      "source": [
        "### Encoding categorical data"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "le5MJreAbW52"
      },
      "source": [
        "Label Encoding the \"Gender\" column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "PxVKWXxLbczC"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "X[:, 2] = le.fit_transform(X[:, 2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "colab_type": "code",
        "id": "-M1KboxFb6OO",
        "outputId": "e2b8c7e8-0cbc-4cdf-f4eb-7f0853a00b88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[619 'France' 0 ... 1 1 101348.88]\n",
            " [608 'Spain' 0 ... 0 1 112542.58]\n",
            " [502 'France' 0 ... 1 0 113931.57]\n",
            " ...\n",
            " [709 'France' 0 ... 0 1 42085.58]\n",
            " [772 'Germany' 1 ... 1 0 92888.52]\n",
            " [792 'France' 0 ... 1 0 38190.78]]\n"
          ]
        }
      ],
      "source": [
        "print(X)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CUxGZezpbMcb"
      },
      "source": [
        "One Hot Encoding the \"Geography\" column"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ColumnTransformer, sklearn kütüphanesinde bulunan bir sınıftır ve makine öğrenimi projelerinde sütun (kolon) bazında dönüşümler uygulamak için kullanılır. Bu sınıf, farklı sütunlara farklı dönüşümler uygulamak ve ardından bu dönüşümleri birleştirmek için kullanışlıdır.\n",
        "ColumnTransformer sınıfı, bu dönüşümleri belirli sütunlara uygulamak için kullanılır. Örnekteki transformers parametresi, dönüşümün adını ('encoder') ve hangi dönüşümün uygulanacağını (OneHotEncoder) belirtir. [1] ise dönüşümün hangi sütuna uygulanacağını belirtir. Burada, 1. sütuna (indeksleme 0'dan başladığı için) OneHotEncoder dönüşümü uygulanacaktır. Yani, bu sütun kategorik bir sütundur ve One-Hot Encoding işlemi uygulanarak kategorik değerler sayısal değerlere dönüştürülecektir.\n",
        "\n",
        "remainder parametresi ise dönüşüm uygulanmayan sütunların ne yapılacağını belirtir. 'passthrough' değeri, dönüşüm uygulanmayan sütunların doğrudan geçirileceği anlamına gelir. Yani, dönüşüm uygulanmayan sütunlar için herhangi bir işlem yapılmaz ve veri setinde aynı şekilde kalır.\n",
        "\n",
        "Sonuç olarak, ColumnTransformer sınıfı, birden fazla sütuna farklı dönüşümler uygulamak ve bu dönüşümleri birleştirmek için kullanılır. Örnekteki ColumnTransformer nesnesi, One-Hot Encoding dönüşümünü belirli bir sütuna uygularken diğer sütunları olduğu gibi bırakır. Böylece, veri kümesi farklı dönüşümler uygulanarak makine öğrenimi algoritması için uygun hale getirilir."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "AMXC8-KMVirw"
      },
      "outputs": [],
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [1])], remainder='passthrough')\n",
        "X = np.array(ct.fit_transform(X))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "colab_type": "code",
        "id": "ZcxwEon-b8nV",
        "outputId": "23a98af4-5e33-4b26-c27b-f06e3c5d2baf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1.0 0.0 0.0 ... 1 1 101348.88]\n",
            " [0.0 0.0 1.0 ... 0 1 112542.58]\n",
            " [1.0 0.0 0.0 ... 1 0 113931.57]\n",
            " ...\n",
            " [1.0 0.0 0.0 ... 0 1 42085.58]\n",
            " [0.0 1.0 0.0 ... 1 0 92888.52]\n",
            " [1.0 0.0 0.0 ... 1 0 38190.78]]\n"
          ]
        }
      ],
      "source": [
        "print(X)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vHol938cW8zd"
      },
      "source": [
        "### Splitting the dataset into the Training set and Test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Z-TDt0Y_XEfc"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RE_FcHyfV3TQ"
      },
      "source": [
        "### Feature Scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "ViCrE00rV8Sk"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-zfEzkRVXIwF"
      },
      "source": [
        "## Part 2 - Building the ANN"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KvdeScabXtlB"
      },
      "source": [
        "### Initializing the ANN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "3dtrScHxXQox"
      },
      "outputs": [],
      "source": [
        "ann = tf.keras.models.Sequential()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rP6urV6SX7kS"
      },
      "source": [
        "### Adding the input layer and the first hidden layer"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Bu kod, Keras kütüphanesinde tanımlanmış bir yapay sinir ağı modeli olan \"ann\" modeline, yeni bir yoğun katman (Dense layer) ekler. Eklenen katman, 6 adet nöron içerir ve bu nöronlar, ReLU (Rectified Linear Unit) olarak adlandırılan aktivasyon fonksiyonunu kullanır.\n",
        "\n",
        "Yoğun katman, yapay sinir ağı modellerinde en sık kullanılan katman türlerinden biridir. Bu katman, önceki katmandaki tüm nöronlarla bağlantılıdır ve her bir nöron, girdi özellikleriyle ağırlıklarını çarparak bir çıktı değeri üretir. Bu çıktı değerleri, sonraki katmana aktarılır ve bu işlem, modelin çıktısına kadar devam eder.\n",
        "\n",
        "ReLU aktivasyon fonksiyonu, en yaygın kullanılan aktivasyon fonksiyonlarından biridir ve genellikle yapay sinir ağı modellerindeki performansı arttırır. ReLU, negatif girdilerin çıktısını sıfır yaparken, pozitif girdilerin aynı şekilde aktarılmasını sağlar. Bu nedenle, modelin daha doğru sonuçlar vermesine yardımcı olabilir."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "bppGycBXYCQr"
      },
      "outputs": [],
      "source": [
        "ann.add(tf.keras.layers.Dense(units=6, activation='relu'))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BELWAc_8YJze"
      },
      "source": [
        "### Adding the second hidden layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "JneR0u0sYRTd"
      },
      "outputs": [],
      "source": [
        "ann.add(tf.keras.layers.Dense(units=6, activation='relu'))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OyNEe6RXYcU4"
      },
      "source": [
        "### Adding the output layer"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Bu kod, \"ann\" modeline bir başka yoğun katman daha ekler. Eklenen katman, sadece 1 adet nöron içerir ve bu nöron, sigmoid olarak adlandırılan aktivasyon fonksiyonunu kullanır.\n",
        "\n",
        "Sigmoid fonksiyonu, girdileri 0 ile 1 arasında bir değere dönüştüren bir aktivasyon fonksiyonudur. Bu fonksiyon, genellikle ikili sınıflandırma problemlerinde kullanılır, çünkü sigmoid fonksiyonunun çıktısı, 0.5'in altında veya üstünde olup olmadığına bağlı olarak, sınıflandırılan örneğin pozitif veya negatif olarak etiketlenmesine karar vermek için kullanılabilir.\n",
        "\n",
        "Yani, eklenen bu son yoğun katman, modelin ikili sınıflandırma problemleri için kullanılmasını sağlar. Sigmoid fonksiyonu, aynı zamanda çıktı değerlerinin olasılık değerlerine dönüştürülmesine de yardımcı olabilir, bu da modelin tahminlerini yorumlamayı daha kolay hale getirir."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Cn3x41RBYfvY"
      },
      "outputs": [],
      "source": [
        "ann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JT4u2S1_Y4WG"
      },
      "source": [
        "## Part 3 - Training the ANN"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8GWlJChhY_ZI"
      },
      "source": [
        "### Compiling the ANN"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "Bu kod, \"ann\" modelinin derlenmesini sağlar. Derleme, modelin eğitim için hazır hale getirilmesini sağlar.\n",
        "\n",
        "Parametrelerin anlamları:\n",
        "\n",
        "\"optimizer\": Modelin optimize edilmesi için kullanılacak algoritmayı belirtir. \"adam\" optimizer, stokastik gradyan inişi algoritması (SGD) temelli bir optimizerdir ve genellikle birçok makine öğrenmesi uygulamasında iyi sonuçlar verir.\n",
        "\"loss\": Modelin kayıp fonksiyonunu belirtir. Binary cross-entropy kaybı, ikili sınıflandırma problemleri için yaygın olarak kullanılan bir kayıp fonksiyonudur.\n",
        "\"metrics\": Modelin performansını ölçmek için kullanılacak metrikleri belirtir. Bu örnekte, sınıflandırma doğruluğu (accuracy) kullanılır.\n",
        "\"ann\" modeli bu şekilde derlenince, artık eğitim işlemine başlamak için hazır hale gelir. Eğitim sürecinde, model, verilen eğitim verilerine göre optimize edilir ve en uygun ağırlık değerleri bulunmaya çalışılır. Bu, modelin daha sonra test verileri üzerinde iyi bir performans göstermesini sağlar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "fG3RrwDXZEaS"
      },
      "outputs": [],
      "source": [
        "ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0QR_G5u7ZLSM"
      },
      "source": [
        "### Training the ANN on the Training set"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "epochs, modelin tüm eğitim verilerini kaç kez göreceği sayısını ifade eder. Bir epoch, tüm eğitim verilerinin bir kez işlenmesine denk gelir. Daha fazla epoch, modelin daha fazla eğitim verisiyle çalışmasına ve daha iyi sonuçlar elde etmesine yardımcı olabilir. Ancak aşırı öğrenme (overfitting) riski de artabilir.\n",
        "\n",
        "batch_size, eğitim sırasında verilerin küçük gruplara bölünmesini sağlar. Bu, modelin bir seferde tüm verileri işlememesini ve belleği tüketmemesini sağlar. Ayrıca, daha küçük bir batch size, modelin daha hızlı öğrenmesine de yardımcı olabilir. Ancak çok küçük bir batch size, gradyan hesaplamalarındaki gürültü seviyesini artırabilir ve sonuçta daha yavaş bir eğitim sürecine neden olabilir.\n",
        "\n",
        "Bu fonksiyonun tam olarak nasıl çalıştığı modelin kütüphanesine göre değişebilir, ancak genel olarak, fit fonksiyonu, modelin öğrenme parametrelerini (ağırlıklar ve biaslar gibi) eğitim verilerine göre ayarlar ve doğru sonuçları vermek için bu parametreleri optimize eder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "colab_type": "code",
        "id": "nHZ-LKv_ZRb3",
        "outputId": "718cc4b0-b5aa-40f0-9b20-d3d31730a531"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "250/250 [==============================] - 1s 721us/step - loss: 0.5466 - accuracy: 0.7476\n",
            "Epoch 2/30\n",
            "250/250 [==============================] - 0s 720us/step - loss: 0.4604 - accuracy: 0.7960\n",
            "Epoch 3/30\n",
            "250/250 [==============================] - 0s 715us/step - loss: 0.4476 - accuracy: 0.7960\n",
            "Epoch 4/30\n",
            "250/250 [==============================] - 0s 803us/step - loss: 0.4398 - accuracy: 0.7960\n",
            "Epoch 5/30\n",
            "250/250 [==============================] - 0s 727us/step - loss: 0.4349 - accuracy: 0.7960\n",
            "Epoch 6/30\n",
            "250/250 [==============================] - 0s 714us/step - loss: 0.4317 - accuracy: 0.7960\n",
            "Epoch 7/30\n",
            "250/250 [==============================] - 0s 708us/step - loss: 0.4290 - accuracy: 0.7960\n",
            "Epoch 8/30\n",
            "250/250 [==============================] - 0s 717us/step - loss: 0.4264 - accuracy: 0.7960\n",
            "Epoch 9/30\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.4239 - accuracy: 0.7964\n",
            "Epoch 10/30\n",
            "250/250 [==============================] - 0s 803us/step - loss: 0.4211 - accuracy: 0.8040\n",
            "Epoch 11/30\n",
            "250/250 [==============================] - 0s 732us/step - loss: 0.4181 - accuracy: 0.8165\n",
            "Epoch 12/30\n",
            "250/250 [==============================] - 0s 799us/step - loss: 0.4155 - accuracy: 0.8250\n",
            "Epoch 13/30\n",
            "250/250 [==============================] - 0s 741us/step - loss: 0.4126 - accuracy: 0.8286\n",
            "Epoch 14/30\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.4102 - accuracy: 0.8301\n",
            "Epoch 15/30\n",
            "250/250 [==============================] - 0s 739us/step - loss: 0.4072 - accuracy: 0.8334\n",
            "Epoch 16/30\n",
            "250/250 [==============================] - 0s 952us/step - loss: 0.4048 - accuracy: 0.8349\n",
            "Epoch 17/30\n",
            "250/250 [==============================] - 0s 739us/step - loss: 0.4027 - accuracy: 0.8351\n",
            "Epoch 18/30\n",
            "250/250 [==============================] - 0s 971us/step - loss: 0.4011 - accuracy: 0.8355\n",
            "Epoch 19/30\n",
            "250/250 [==============================] - 0s 814us/step - loss: 0.3991 - accuracy: 0.8369\n",
            "Epoch 20/30\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3981 - accuracy: 0.8370\n",
            "Epoch 21/30\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3968 - accuracy: 0.8364\n",
            "Epoch 22/30\n",
            "250/250 [==============================] - 0s 759us/step - loss: 0.3959 - accuracy: 0.8369\n",
            "Epoch 23/30\n",
            "250/250 [==============================] - 0s 880us/step - loss: 0.3949 - accuracy: 0.8363\n",
            "Epoch 24/30\n",
            "250/250 [==============================] - 0s 731us/step - loss: 0.3934 - accuracy: 0.8363\n",
            "Epoch 25/30\n",
            "250/250 [==============================] - 0s 731us/step - loss: 0.3921 - accuracy: 0.8363\n",
            "Epoch 26/30\n",
            "250/250 [==============================] - 0s 719us/step - loss: 0.3906 - accuracy: 0.8360\n",
            "Epoch 27/30\n",
            "250/250 [==============================] - 0s 948us/step - loss: 0.3882 - accuracy: 0.8384\n",
            "Epoch 28/30\n",
            "250/250 [==============================] - 0s 789us/step - loss: 0.3861 - accuracy: 0.8393\n",
            "Epoch 29/30\n",
            "250/250 [==============================] - 0s 896us/step - loss: 0.3817 - accuracy: 0.8436\n",
            "Epoch 30/30\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3756 - accuracy: 0.8450\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x12045d8e890>"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ann.fit(X_train, y_train, batch_size = 32, epochs = 30)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tJj5k2MxZga3"
      },
      "source": [
        "## Part 4 - Making the predictions and evaluating the model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "84QFoqGYeXHL"
      },
      "source": [
        "### Predicting the result of a single observation"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CGRo3eacgDdC"
      },
      "source": [
        "**Homework**\n",
        "\n",
        "Use our ANN model to predict if the customer with the following informations will leave the bank: \n",
        "\n",
        "Geography: France\n",
        "\n",
        "Credit Score: 600\n",
        "\n",
        "Gender: Male\n",
        "\n",
        "Age: 40 years old\n",
        "\n",
        "Tenure: 3 years\n",
        "\n",
        "Balance: \\$ 60000\n",
        "\n",
        "Number of Products: 2\n",
        "\n",
        "Does this customer have a credit card ? Yes\n",
        "\n",
        "Is this customer an Active Member: Yes\n",
        "\n",
        "Estimated Salary: \\$ 50000\n",
        "\n",
        "So, should we say goodbye to that customer ?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZhU1LTgPg-kH"
      },
      "source": [
        "**Solution**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Bu kod, eğitilmiş bir yapay sinir ağı modeli kullanılarak yeni bir müşterinin kredi ödeme olasılığını tahmin eder.\n",
        "\n",
        "İlk olarak, sc.transform fonksiyonu kullanılarak veri normalleştirilir. Bu, verilerin model tarafından kullanılabilmesi için aynı ölçekte olmasını sağlar.\n",
        "\n",
        "Sonra, normalleştirilmiş veri, yapay sinir ağı modelinin predict fonksiyonuna beslenir. Model, müşterinin kredi ödeme olasılığını tahmin etmek için bu verileri kullanır.\n",
        "\n",
        "Son olarak, 0.5'ten büyükse 1, küçükse 0 olarak yeniden düzenlenmiş tahmin değeri ekrana yazdırılır. Bu, müşterinin kredi ödeme olasılığının kabaca %50'ye yakın olup olmadığını gösterir. Tahmin değeri, belirli bir kesme değerine göre ikili sınıflandırma olarak belirlenir."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "2d8IoCCkeWGL",
        "outputId": "957f3970-e197-4c3b-a150-7f69dc567f5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 28ms/step\n",
            "[[ True]]\n"
          ]
        }
      ],
      "source": [
        "print(ann.predict(sc.transform([[1, 0, 0, 600, 1, 40, 10, 6000000, 2, 1, 1, 5000000]])) > 0.5)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wGjx94g2n7OV"
      },
      "source": [
        "Therefore, our ANN model predicts that this customer stays in the bank!\n",
        "\n",
        "**Important note 1:** Notice that the values of the features were all input in a double pair of square brackets. That's because the \"predict\" method always expects a 2D array as the format of its inputs. And putting our values into a double pair of square brackets makes the input exactly a 2D array.\n",
        "\n",
        "**Important note 2:** Notice also that the \"France\" country was not input as a string in the last column but as \"1, 0, 0\" in the first three columns. That's because of course the predict method expects the one-hot-encoded values of the state, and as we see in the first row of the matrix of features X, \"France\" was encoded as \"1, 0, 0\". And be careful to include these values in the first three columns, because the dummy variables are always created in the first columns."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "u7yx47jPZt11"
      },
      "source": [
        "### Predicting the Test set results"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Bu kod, eğitilmiş bir yapay sinir ağı (Artificial Neural Network) modelini kullanarak tahminler yapar ve sonuçları gerçek etiketlerle karşılaştırır.\n",
        "\n",
        "İkinci satır, y_pred dizisindeki tahmin değerlerini ikili (binary) formda, yani 0.5'ten büyükse 1, küçükse 0 olarak yeniden düzenler. Bu, modelin iki sınıf arasında bir ayrım yaptığı varsayımına dayanır.\n",
        "\n",
        "Üçüncü satır, np.concatenate fonksiyonunu kullanarak, y_pred ve gerçek test etiketleri y_test arasında birleştirilmiş bir matris oluşturur. Bu matris, her bir örneğin tahmin değeri ve gerçek etiketleri ile birlikte görüntülenir. reshape fonksiyonu, her bir matrisin bir sütunlu bir diziye dönüştürülmesine yardımcı olur."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "colab_type": "code",
        "id": "nIyEeQdRZwgs",
        "outputId": "82330ba8-9bdc-4fd1-d3cf-b6d78ee7c2a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "63/63 [==============================] - 0s 1ms/step\n",
            "[[0 0]\n",
            " [0 1]\n",
            " [0 0]\n",
            " ...\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 0]]\n"
          ]
        }
      ],
      "source": [
        "y_pred = ann.predict(X_test)\n",
        "y_pred = (y_pred > 0.5)\n",
        "print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "o0oyfLWoaEGw"
      },
      "source": [
        "### Making the Confusion Matrix"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "confusion_matrix fonksiyonunu kullanarak gerçek test etiketleri y_test ve tahmin edilen test etiketleri y_pred arasında bir karışıklık matrisi oluşturur. Karışıklık matrisi, sınıflandırma problemlerinde sınıflandırma sonuçlarının doğruluğunu gösteren bir matristir."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "colab_type": "code",
        "id": "ci6K_r6LaF6P",
        "outputId": "4d854e9e-22d5-432f-f6e5-a102fe3ae0bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1533   62]\n",
            " [ 227  178]]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.8555"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(cm)\n",
        "accuracy_score(y_test, y_pred)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "artificial_neural_network.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
